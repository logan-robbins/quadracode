"""Provides a suite of LangChain tools for high-level autonomous operation control.

This module defines a set of tools that enable an agent operating in a fully
autonomous mode (`HUMAN_OBSOLETE`) to manage its own lifecycle, report progress,
escalate critical issues, and request final reviews. These tools are the primary
interface for the agent's meta-cognitive functions, allowing it to structure its
work into milestones, critique its own hypotheses, and adhere to a strict,
test-driven process before handing off its work. The events generated by these
tools are critical for observability and for triggering human intervention when
necessary.
"""
from __future__ import annotations

import json
from typing import List, Optional, Literal

from langchain_core.tools import tool
from pydantic import BaseModel, Field

from quadracode_contracts import (
    AutonomousCheckpointRecord,
    AutonomousEscalationRecord,
    AutonomousRoutingDirective,
    HypothesisCritiqueRecord,
)

from .test_suite import execute_full_test_suite


class AutonomousCheckpointRequest(BaseModel):
    """Schema for recording autonomous milestone checkpoints.

    This schema structures progress reports from a fully autonomous agent. It allows
    the agent to communicate its status (`in_progress`, `complete`, `blocked`) for
    a given `milestone`, provide a `summary` of its work, and outline its `next_steps`.
    This creates a transparent audit trail of the agent's long-running tasks.
    """

    milestone: int = Field(..., ge=1)
    status: Literal["in_progress", "complete", "blocked"]
    summary: str = Field(..., min_length=1)
    next_steps: List[str] = Field(default_factory=list)
    title: Optional[str] = Field(
        default=None,
        description="Optional human-readable milestone name.",
    )


class AutonomousEscalationRequest(BaseModel):
    """Schema for escalating a blocking issue to a human supervisor.

    When an autonomous agent encounters an error it cannot resolve, it uses this
    schema to request human intervention. It must provide a classification of the
    error (`error_type`), a detailed `description`, and a list of `recovery_attempts`.
    The `is_fatal` flag signals that the agent is permanently stuck and requires
    external help.
    """

    error_type: str = Field(..., min_length=1)
    description: str = Field(..., min_length=1)
    recovery_attempts: List[str] = Field(default_factory=list)
    is_fatal: bool = Field(
        default=True,
        description="Whether this error is fatal and requires human intervention.",
    )


class HypothesisCritiqueRequest(BaseModel):
    """Schema for a structured, self-generated critique of a problem-solving hypothesis.

    This schema supports the agent's meta-cognitive loop. After pursuing a particular
    `hypothesis` (identified by its `cycle_id`), the agent can use this tool to record
    a structured critique of its own approach. This includes a `summary`, qualitative
    `feedback`, a `category` (e.g., 'code_quality'), and a `severity` level, fostering
    a continuous learning and improvement process.
    """

    cycle_id: str = Field(..., min_length=1)
    hypothesis: str = Field(..., min_length=1)
    critique_summary: str = Field(..., min_length=1)
    qualitative_feedback: str = Field(..., min_length=1)
    category: Literal["code_quality", "architecture", "test_coverage", "performance"]
    severity: Literal["low", "moderate", "high", "critical"]
    evidence: List[str] = Field(default_factory=list)


class FinalReviewRequest(BaseModel):
    """Schema for submitting completed work for final human review.

    This schema enforces a strict quality gate. Before an agent can request a
    final review, it MUST successfully execute the full test suite. The request
    includes a `summary` of the completed work, a list of relevant `artifacts`,
    and can enforce a minimum code `coverage_goal`. This ensures that human
    reviewers are only engaged when the work meets a baseline level of quality.
    """

    summary: str = Field(..., min_length=1)
    recovery_attempts: List[str] = Field(default_factory=list)
    artifacts: List[str] = Field(
        default_factory=list,
        description="Artifacts that should be highlighted for HumanClone review.",
    )
    workspace_root: Optional[str] = Field(
        default=None,
        description="Workspace root override for the test suite tool.",
    )
    include_e2e: bool = Field(
        default=True,
        description="Run end-to-end suites if discovery finds them.",
    )
    coverage_goal: Optional[float] = Field(
        default=None,
        ge=0,
        le=100,
        description="Optional coverage threshold to assert before final review.",
    )


def _format_output(payload: dict[str, object]) -> str:
    """Serializes a dictionary to a formatted JSON string for consistent tool output."""
    return json.dumps(payload, indent=2, sort_keys=True)


@tool(args_schema=AutonomousCheckpointRequest)
def autonomous_checkpoint(
    milestone: int,
    status: str,
    summary: str,
    next_steps: List[str] | None = None,
    title: str | None = None,
) -> str:
    """Records a structured progress update for a specific milestone in an autonomous task.

    This tool allows an agent to signal its progress through a multi-step plan.
    By recording checkpoints, the agent provides observability into its status,
    what it has just accomplished, and what it intends to do next. This is a key
    component of the `HUMAN_OBSOLETE` operational mode, enabling asynchronous
    monitoring of complex tasks.
    """

    record = AutonomousCheckpointRecord(
        milestone=milestone,
        status=status,  # type: ignore[arg-type]
        summary=summary,
        next_steps=next_steps or [],
        title=title,
    )
    return _format_output(
        {
            "event": "checkpoint",
            "record": record.dict(),
        }
    )


@tool(args_schema=AutonomousEscalationRequest)
def autonomous_escalate(
    error_type: str,
    description: str,
    recovery_attempts: List[str] | None = None,
    is_fatal: bool = True,
) -> str:
    """Signals that an autonomous agent is blocked and requires human intervention.

    When an agent determines it cannot make further progress due to an unrecoverable
    error, it uses this tool to escalate the issue. The tool packages the error
    details into a structured `AutonomousEscalationRecord` and, if the error is
    marked as `is_fatal`, generates a routing directive to deliver the message to
    a human supervisor.
    """

    payload = AutonomousEscalationRequest(
        error_type=error_type,
        description=description,
        recovery_attempts=recovery_attempts or [],
        is_fatal=is_fatal,
    )

    if not payload.is_fatal:
        return _format_output(
            {
                "event": "escalation",
                "status": "dismissed",
                "message": "Error is not fatal. Continue autonomous recovery.",
            }
        )

    record = AutonomousEscalationRecord(
        error_type=payload.error_type,
        description=payload.description,
        recovery_attempts=payload.recovery_attempts,
        is_fatal=True,
    )
    routing = AutonomousRoutingDirective(
        deliver_to_human=True,
        escalate=True,
        reason=payload.description,
        recovery_attempts=payload.recovery_attempts,
    )
    return _format_output(
        {
            "event": "escalation",
            "status": "escalate",
            "record": record.dict(),
            "routing": routing.to_payload(),
        }
    )


@tool(args_schema=HypothesisCritiqueRequest)
def hypothesis_critique(
    cycle_id: str,
    hypothesis: str,
    critique_summary: str,
    qualitative_feedback: str,
    category: str,
    severity: str,
    evidence: List[str] | None = None,
) -> str:
    """Allows an agent to perform a structured self-critique of its own hypothesis.

    This tool is central to the agent's ability to learn from its mistakes. After
    a plan-replan-propose (PRP) cycle, the agent can use this tool to analyze its
    previous hypothesis and document flaws in its reasoning or execution. This
    feedback is recorded and can be used to inform future planning cycles,
    preventing the agent from repeating the same errors.
    """

    record = HypothesisCritiqueRecord(
        cycle_id=cycle_id,
        hypothesis=hypothesis,
        critique_summary=critique_summary,
        qualitative_feedback=qualitative_feedback,
        category=category,  # type: ignore[arg-type]
        severity=severity,  # type: ignore[arg-type]
        evidence=evidence or [],
    )
    return _format_output(
        {
            "event": "hypothesis_critique",
            "record": record.dict(),
        }
    )


@tool(args_schema=FinalReviewRequest)
def request_final_review(
    summary: str,
    recovery_attempts: List[str] | None = None,
    artifacts: List[str] | None = None,
    workspace_root: str | None = None,
    include_e2e: bool = True,
    coverage_goal: float | None = None,
) -> str:
    """Initiates the final handoff process by running all tests and submitting for review.

    This tool serves as a critical quality gate for autonomous work. It first
    triggers a full run of the project's test suite via the `execute_full_test_suite`
    function. Ifâ€”and only ifâ€”all tests pass, it packages a summary of the work,
    a list of generated artifacts, and the test results into a final review request.
    If a `coverage_goal` is set, it also verifies that the minimum code coverage
    was achieved. If the tests fail, the request is rejected, forcing the agent to
    continue its work.
    """

    tests = execute_full_test_suite(
        workspace_root=workspace_root,
        include_e2e=include_e2e,
    )
    overall_status = str(tests.get("overall_status") or "").lower()
    if overall_status != "passed":
        return _format_output(
            {
                "event": "tests_failed",
                "message": "Full test suite must pass before requesting final review.",
                "tests": tests,
            }
        )

    record = AutonomousEscalationRecord(
        error_type="final_review",
        description=summary,
        recovery_attempts=recovery_attempts or [],
        is_fatal=False,
    )
    payload: dict[str, object] = {
        "event": "final_review_request",
        "record": record.dict(),
        "tests": tests,
        "artifacts": artifacts or [],
    }

    coverage_data = tests.get("coverage") if isinstance(tests, dict) else None
    if coverage_goal is not None and isinstance(coverage_data, dict):
        coverage_min = coverage_data.get("min")
        if isinstance(coverage_min, (int, float)):
            payload["coverage_goal_met"] = coverage_min >= coverage_goal

    return _format_output(payload)
